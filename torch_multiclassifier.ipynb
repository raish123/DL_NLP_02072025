{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8a93657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the important library or module which is used in this project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "912e1c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>205</td>\n",
       "      <td>196</td>\n",
       "      <td>213</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>213</td>\n",
       "      <td>203</td>\n",
       "      <td>174</td>\n",
       "      <td>151</td>\n",
       "      <td>188</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      9       0       0       0       0       0       0       0       0   \n",
       "1      7       0       0       0       0       0       0       0       0   \n",
       "2      0       0       0       0       0       0       1       0       0   \n",
       "3      8       0       0       0       0       0       0       0       0   \n",
       "4      8       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         7         0        50       205       196   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...       142       142       142        21         0         3   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...       213       203       174       151       188        10   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       213       165         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the fashion amnist dataset\n",
    "data = pd.read_csv('fmnist_small.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf0fcaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "6    656\n",
       "7    620\n",
       "5    612\n",
       "2    604\n",
       "4    595\n",
       "0    595\n",
       "3    591\n",
       "1    590\n",
       "8    582\n",
       "9    555\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if i want to see the target variable counts\n",
    "data.label.value_counts()  #this case come under the multi classifier category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c3ce213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train tets split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data.drop(\"label\",axis=1)\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0e49e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking gpu avilable in system or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6662ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now splitting them into training and testing set\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8ae2d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4800, 784), (1200, 784), (4800,), (1200,), 2, 2, 1, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape,x_train.ndim,x_test.ndim,y_train.ndim,y_test.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5366b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0), np.int64(0))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.pixel1.min(),x_train.pixel1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93ceefee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now doing input scaling and applying min max scaling to inputs\n",
    "x_train = x_train/255.0\n",
    "x_test  = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b175488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, dtype('float64'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train),x_train.iloc[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae48fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing training and testing dataset to numpy array object\n",
    "x_train_np = np.array(x_train)\n",
    "x_test_np =  np.array(x_test)\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np =  np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7feab19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " dtype('float64'),\n",
       " 2,\n",
       " dtype('float64'),\n",
       " 1,\n",
       " dtype('int64'),\n",
       " 1,\n",
       " dtype('int64'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_np.ndim,x_train_np.dtype,x_test_np.ndim,x_test_np.dtype,y_train_np.ndim,y_train_np.dtype,y_test_np.ndim,y_test_np.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984bf55",
   "metadata": {},
   "source": [
    "# in case of multi classifier pblm statement inputs tensor dtype will be float32 and output varibale will be long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d842eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now converting pandas input and output df objects  to tensor object\n",
    "x_train_tensor = torch.tensor(x_train_np,dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_np,dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np,dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_np,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf9f9f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, torch.int64, torch.int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor.dtype,x_test_tensor.dtype,y_train_tensor.dtype,y_test_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac8b8e",
   "metadata": {},
   "source": [
    "# using dataset class i am creating custom class that split data into batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "150ecebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    #creating constructor method in that we r initializing the instance variable in it.\n",
    "    def __init__(self,input_data,output_data):\n",
    "        #calling parent class property\n",
    "        super().__init__()\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        \n",
    "        \n",
    "    #using another magical method which show the shape of input data\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    #using another magical method which divide data into batches based on index\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_data[index],self.output_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a161db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now calling the customdatset class\n",
    "training_dataset = CustomDataset(x_train_tensor,y_train_tensor)\n",
    "testing_dataset  = CustomDataset(x_test_tensor,y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce68a9",
   "metadata": {},
   "source": [
    "# now using dataloader class to convert dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6ad0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_set = DataLoader(training_dataset,batch_size=8,shuffle=True,pin_memory=True)\n",
    "test_dataloader_set = DataLoader(testing_dataset,batch_size=8,shuffle=True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5790daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3608, 0.3529, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1451, 0.1333, 0.0039]])\n",
      "tensor([7, 5, 3, 8, 7, 1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "for feature,label in train_dataloader_set:\n",
    "    print(feature)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b757e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 784])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef26c23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c323bab",
   "metadata": {},
   "source": [
    "# defining the Neural network fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4599384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "\n",
    "class MyNeuralNetworkLayer(nn.Module):\n",
    "    \n",
    "    #using constructor method for defining the architecture of neural network !!!\n",
    "    def __init__(self,input_feature_shape):\n",
    "        #calling the parent class constructor inheriting the property from it.\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = Sequential(\n",
    "            \n",
    "            #defining the 1st hidden layer neurons\n",
    "            nn.Linear(input_feature_shape,512), #index=0\n",
    "            nn.BatchNorm1d(512),                #index=1\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            \n",
    "            #efining the 2nd hidden layer neurons\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            #defining the 3rd hidden layer neurons\n",
    "            nn.Linear(256,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            #defining the output layer\n",
    "            nn.Linear(64,10)\n",
    "            #NOTE: No Softmax here → CrossEntropyLoss expects raw logits(value automatically they find probability of each class)\n",
    "         \n",
    "        )\n",
    "    \n",
    "    #creating another method to excute propogation class\n",
    "    def forward(self,input_feature):\n",
    "        return self.model(input_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5895cd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNeuralNetworkLayer(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "    (12): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating an object of custom neural network class\n",
    "inp_shape = x_train_tensor.shape[1]\n",
    "model = MyNeuralNetworkLayer(inp_shape).to(device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac56caa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyNeuralNetworkLayer                     --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       401,920\n",
       "│    └─BatchNorm1d: 2-2                  1,024\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─Dropout: 2-4                      --\n",
       "│    └─Linear: 2-5                       131,328\n",
       "│    └─BatchNorm1d: 2-6                  512\n",
       "│    └─ReLU: 2-7                         --\n",
       "│    └─Dropout: 2-8                      --\n",
       "│    └─Linear: 2-9                       16,448\n",
       "│    └─BatchNorm1d: 2-10                 128\n",
       "│    └─ReLU: 2-11                        --\n",
       "│    └─Dropout: 2-12                     --\n",
       "│    └─Linear: 2-13                      650\n",
       "=================================================================\n",
       "Total params: 552,010\n",
       "Trainable params: 552,010\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if i wan tto show how many no of trainable parameter trained by model\n",
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2af74aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now defining how many no of iteration want to give at time of training,what will be loss function,optimers\n",
    "\n",
    "loss_fxn = nn.CrossEntropyLoss() #this loss fxn will be same as sparse categorical cross entropy\n",
    "optimizers = torch.optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.01) \n",
    "#optimizer we used to update the trainable paramerter in each layer during Back Propgation time\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1422678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec391f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Hidden Layer Weights (Before Training):\n",
      "Parameter containing:\n",
      "tensor([[-0.0322, -0.0321,  0.0275,  ..., -0.0088, -0.0137, -0.0023],\n",
      "        [ 0.0141, -0.0279, -0.0049,  ..., -0.0106, -0.0333, -0.0009],\n",
      "        [-0.0275, -0.0249,  0.0027,  ..., -0.0016,  0.0085,  0.0260],\n",
      "        ...,\n",
      "        [ 0.0305, -0.0152,  0.0227,  ..., -0.0158, -0.0041, -0.0143],\n",
      "        [ 0.0288, -0.0028, -0.0180,  ..., -0.0218,  0.0216,  0.0021],\n",
      "        [ 0.0315, -0.0085,  0.0047,  ..., -0.0198, -0.0295,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"First Hidden Layer Weights (Before Training):\")\n",
    "print(model.model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70c1df81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Hidden Layer Bias (Before Training):\n",
      "Parameter containing:\n",
      "tensor([-0.0094, -0.0140,  0.0269,  0.0340, -0.0115, -0.0143,  0.0089, -0.0280,\n",
      "         0.0183,  0.0002,  0.0058,  0.0313,  0.0071, -0.0030,  0.0085, -0.0342,\n",
      "         0.0265,  0.0126, -0.0159,  0.0100, -0.0267,  0.0193,  0.0048, -0.0337,\n",
      "        -0.0189,  0.0018, -0.0073,  0.0010, -0.0233, -0.0306, -0.0279,  0.0214,\n",
      "         0.0231, -0.0180, -0.0317, -0.0169, -0.0083,  0.0113, -0.0042, -0.0167,\n",
      "         0.0244,  0.0057, -0.0350, -0.0189, -0.0321, -0.0003, -0.0124,  0.0139,\n",
      "         0.0310,  0.0248, -0.0247,  0.0291,  0.0202, -0.0050,  0.0024, -0.0061,\n",
      "        -0.0109,  0.0125, -0.0285,  0.0313, -0.0163,  0.0016, -0.0272,  0.0236,\n",
      "        -0.0107,  0.0082, -0.0287, -0.0250, -0.0336,  0.0168, -0.0320, -0.0032,\n",
      "        -0.0036,  0.0155, -0.0212, -0.0288, -0.0342,  0.0096,  0.0354, -0.0056,\n",
      "        -0.0262, -0.0302,  0.0061,  0.0124,  0.0172, -0.0159, -0.0350, -0.0172,\n",
      "         0.0311,  0.0059,  0.0106,  0.0104,  0.0017, -0.0242, -0.0268, -0.0197,\n",
      "        -0.0240,  0.0277, -0.0096, -0.0297,  0.0068,  0.0247,  0.0348, -0.0002,\n",
      "        -0.0194,  0.0175,  0.0258, -0.0138, -0.0199, -0.0249, -0.0042,  0.0074,\n",
      "        -0.0052, -0.0261, -0.0182,  0.0151,  0.0147, -0.0137,  0.0190,  0.0111,\n",
      "         0.0122, -0.0281, -0.0075, -0.0088, -0.0158,  0.0080, -0.0217, -0.0170,\n",
      "         0.0132, -0.0351,  0.0082,  0.0071, -0.0340,  0.0162,  0.0238,  0.0295,\n",
      "         0.0049,  0.0086,  0.0312,  0.0128, -0.0067,  0.0214,  0.0175,  0.0165,\n",
      "         0.0158,  0.0083, -0.0183, -0.0258,  0.0348,  0.0189, -0.0150, -0.0324,\n",
      "         0.0201,  0.0223,  0.0006,  0.0106, -0.0224,  0.0238,  0.0357, -0.0255,\n",
      "         0.0106,  0.0200, -0.0289,  0.0085,  0.0002,  0.0065, -0.0037, -0.0088,\n",
      "        -0.0042, -0.0342,  0.0144,  0.0144,  0.0213,  0.0279,  0.0131,  0.0014,\n",
      "        -0.0103, -0.0023, -0.0283,  0.0257,  0.0027,  0.0069,  0.0163,  0.0118,\n",
      "         0.0262,  0.0136, -0.0314, -0.0076, -0.0303, -0.0205,  0.0078, -0.0115,\n",
      "         0.0006, -0.0093, -0.0010, -0.0074,  0.0172,  0.0113, -0.0160, -0.0314,\n",
      "        -0.0234, -0.0181, -0.0103, -0.0147, -0.0312,  0.0263, -0.0223, -0.0329,\n",
      "        -0.0106,  0.0270,  0.0337,  0.0296, -0.0209, -0.0163,  0.0148,  0.0035,\n",
      "        -0.0075,  0.0312, -0.0113, -0.0132,  0.0058,  0.0216,  0.0268, -0.0195,\n",
      "         0.0061,  0.0052,  0.0339,  0.0168,  0.0098,  0.0224, -0.0289, -0.0161,\n",
      "         0.0083,  0.0033,  0.0233, -0.0200,  0.0116,  0.0353, -0.0079,  0.0245,\n",
      "         0.0140,  0.0167, -0.0086, -0.0068, -0.0272, -0.0088,  0.0211,  0.0127,\n",
      "         0.0356,  0.0288, -0.0087,  0.0119, -0.0131,  0.0115,  0.0136,  0.0114,\n",
      "         0.0344, -0.0181,  0.0139,  0.0133,  0.0049,  0.0027, -0.0041,  0.0298,\n",
      "         0.0053, -0.0052, -0.0333,  0.0321, -0.0200,  0.0070, -0.0332,  0.0199,\n",
      "        -0.0185,  0.0017,  0.0308,  0.0283, -0.0268,  0.0046,  0.0221, -0.0221,\n",
      "        -0.0068,  0.0294, -0.0112,  0.0002, -0.0234,  0.0168, -0.0112,  0.0006,\n",
      "        -0.0338, -0.0008, -0.0110,  0.0019, -0.0187, -0.0357, -0.0288, -0.0124,\n",
      "        -0.0256, -0.0299, -0.0087, -0.0147, -0.0306, -0.0265,  0.0231,  0.0209,\n",
      "        -0.0059,  0.0074, -0.0054, -0.0124,  0.0154, -0.0286, -0.0331,  0.0296,\n",
      "        -0.0081, -0.0156,  0.0006,  0.0270,  0.0190,  0.0079, -0.0202,  0.0123,\n",
      "         0.0130,  0.0301,  0.0219,  0.0078, -0.0197,  0.0311, -0.0187,  0.0067,\n",
      "        -0.0113, -0.0064, -0.0168, -0.0065, -0.0340,  0.0312, -0.0179,  0.0124,\n",
      "        -0.0230,  0.0252, -0.0076, -0.0084,  0.0023,  0.0136,  0.0291,  0.0237,\n",
      "         0.0341, -0.0147,  0.0143, -0.0031,  0.0003,  0.0194, -0.0001,  0.0113,\n",
      "        -0.0275,  0.0234, -0.0164,  0.0164, -0.0060, -0.0228, -0.0342,  0.0225,\n",
      "         0.0325,  0.0227, -0.0248,  0.0180,  0.0298, -0.0340, -0.0092,  0.0105,\n",
      "         0.0154,  0.0099,  0.0217,  0.0356,  0.0176, -0.0246, -0.0256,  0.0133,\n",
      "         0.0061,  0.0258,  0.0345,  0.0189,  0.0188, -0.0298, -0.0122,  0.0180,\n",
      "        -0.0355,  0.0203, -0.0263, -0.0319,  0.0066, -0.0038, -0.0343,  0.0210,\n",
      "        -0.0230, -0.0256,  0.0238,  0.0022,  0.0262,  0.0323, -0.0175, -0.0209,\n",
      "        -0.0178, -0.0242,  0.0321,  0.0118,  0.0322,  0.0114, -0.0168, -0.0116,\n",
      "         0.0180, -0.0064,  0.0268, -0.0340,  0.0073, -0.0159,  0.0327, -0.0221,\n",
      "         0.0125,  0.0245,  0.0240,  0.0143, -0.0261,  0.0121,  0.0254,  0.0090,\n",
      "        -0.0201, -0.0315,  0.0032,  0.0309, -0.0107,  0.0155, -0.0166,  0.0227,\n",
      "         0.0021,  0.0328,  0.0124, -0.0115,  0.0225, -0.0291, -0.0250,  0.0329,\n",
      "        -0.0180,  0.0024,  0.0307,  0.0187,  0.0348,  0.0129,  0.0336,  0.0273,\n",
      "         0.0254, -0.0337, -0.0067,  0.0127, -0.0003,  0.0260,  0.0328,  0.0336,\n",
      "         0.0291,  0.0193,  0.0074,  0.0045, -0.0122,  0.0307,  0.0213,  0.0172,\n",
      "        -0.0134, -0.0173,  0.0337,  0.0216, -0.0329,  0.0100,  0.0225,  0.0258,\n",
      "        -0.0351,  0.0293, -0.0173,  0.0097, -0.0130,  0.0027, -0.0065, -0.0264,\n",
      "        -0.0095, -0.0201,  0.0280, -0.0104,  0.0060, -0.0196,  0.0153, -0.0126,\n",
      "         0.0292,  0.0194, -0.0227,  0.0211,  0.0353,  0.0339, -0.0192, -0.0168,\n",
      "        -0.0079, -0.0173,  0.0272, -0.0263, -0.0355,  0.0094,  0.0159,  0.0168,\n",
      "        -0.0069,  0.0174,  0.0316, -0.0185, -0.0246,  0.0162,  0.0063, -0.0332],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"First Hidden Layer Bias (Before Training):\")\n",
    "print(model.model[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53fe5bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Hidden Layer Weights (Before Training):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0143,  0.0048, -0.0227,  ...,  0.0366, -0.0319, -0.0414],\n",
      "        [ 0.0214, -0.0232, -0.0153,  ..., -0.0199,  0.0111, -0.0068],\n",
      "        [ 0.0368, -0.0386, -0.0036,  ..., -0.0159,  0.0251, -0.0044],\n",
      "        ...,\n",
      "        [-0.0310,  0.0065, -0.0295,  ..., -0.0144, -0.0046,  0.0434],\n",
      "        [ 0.0272, -0.0030,  0.0352,  ..., -0.0073, -0.0408,  0.0396],\n",
      "        [ 0.0242, -0.0428,  0.0340,  ..., -0.0009,  0.0297,  0.0386]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Second Hidden Layer Weights (Before Training):\")\n",
    "print(model.model[4].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abac47a",
   "metadata": {},
   "source": [
    "# now we will start the training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86a0aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.6299, Test Loss: 1.1809\n",
      "Epoch [2/10], Train Loss: 1.2177, Test Loss: 0.9612\n",
      "Epoch [3/10], Train Loss: 1.0627, Test Loss: 0.8029\n",
      "Epoch [4/10], Train Loss: 0.9657, Test Loss: 0.7396\n",
      "Epoch [5/10], Train Loss: 0.8708, Test Loss: 0.6383\n",
      "Epoch [6/10], Train Loss: 0.8160, Test Loss: 0.6456\n",
      "Epoch [7/10], Train Loss: 0.7915, Test Loss: 0.5991\n",
      "Epoch [8/10], Train Loss: 0.7559, Test Loss: 0.5836\n",
      "Epoch [9/10], Train Loss: 0.7140, Test Loss: 0.5616\n",
      "Epoch [10/10], Train Loss: 0.7115, Test Loss: 0.5575\n"
     ]
    }
   ],
   "source": [
    "avg_training_loss,avg_testing_loss = [],[]\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    training_loss_count = 0  # <- Reset for each epoch\n",
    "    \n",
    "    #first we load tha dataset and passing them into neural architecture \n",
    "    for train_feature,train_label in train_dataloader_set:\n",
    "        #now laoding the data into GPU\n",
    "        train_feature = train_feature.to(device)\n",
    "        train_label   = train_label.to(device)\n",
    "        \n",
    "        #now passing the training feature to neural netowrk architecture the first algorithm that work is forward pass.\n",
    "        #generate the prediction output\n",
    "        y_prediction = model.forward(train_feature)\n",
    "        \n",
    "        #evaluating the performance of model at time of training by calculating the loss value.\n",
    "        losses = loss_fxn(y_prediction,train_label)\n",
    "        \n",
    "        #updating the training_loss_count\n",
    "        training_loss_count = training_loss_count+losses.item()\n",
    "        \n",
    "        #clearing the gradient in optimizers object\n",
    "        optimizers.zero_grad() #before reducing the loss value by using backpropgation algorithm first of we have to clear the gradients which present in optimizers\n",
    "        \n",
    "        #now reducing the loss value by doing back propgation algorithm\n",
    "        losses.backward() #back propgation algorithm will calculate the gradients in each layer of trainaianble parameter wrt to losses\n",
    "        \n",
    "        #updating the trainable parameter by using optimizers\n",
    "        optimizers.step()\n",
    "    \n",
    "    #now calculating average training loss.\n",
    "    avg_train_loss = training_loss_count/len(train_dataloader_set)\n",
    "    avg_training_loss.append(avg_train_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #now validating the testing loss or testing data.\n",
    "    testing_loss_count = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    #while testing the data we have to stop trcking the require gradient not applying overfitting and backpropogation in it.\n",
    "    with torch.no_grad():\n",
    "        for test_feature,test_label in test_dataloader_set:\n",
    "            #now loading the data into GPU\n",
    "            test_feature = test_feature.to(device)\n",
    "            test_label   = test_label.to(device)\n",
    "            \n",
    "            #now passing the test feature into neural network architecture and doing forward pass calculation.\n",
    "            prediction = model.forward(test_feature) #return each batch ke andar each record ke 10 probability\n",
    "            \n",
    "            #calculating the testing loss and updating the testing_loss_count\n",
    "            test_loss = loss_fxn(prediction,test_label)\n",
    "            \n",
    "            testing_loss_count = testing_loss_count+test_loss.item()\n",
    "    \n",
    "        #now calculating average training loss.\n",
    "        avg_test_loss = testing_loss_count/len(test_dataloader_set)\n",
    "        avg_testing_loss.append(avg_test_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e78445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fc8c88d",
   "metadata": {},
   "source": [
    "# to track train accuracy and test accuracy while training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "580d590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.6876, Train Acc: 79.17% | Test Loss: 0.5536, Test Acc: 83.25%\n",
      "Epoch [2/10], Train Loss: 0.6683, Train Acc: 79.54% | Test Loss: 0.5524, Test Acc: 81.67%\n",
      "Epoch [3/10], Train Loss: 0.6514, Train Acc: 79.88% | Test Loss: 0.5221, Test Acc: 84.17%\n",
      "Epoch [4/10], Train Loss: 0.6348, Train Acc: 80.96% | Test Loss: 0.5321, Test Acc: 83.00%\n",
      "Epoch [5/10], Train Loss: 0.6158, Train Acc: 80.79% | Test Loss: 0.5218, Test Acc: 82.92%\n",
      "Epoch [6/10], Train Loss: 0.6218, Train Acc: 80.21% | Test Loss: 0.5061, Test Acc: 83.08%\n",
      "Epoch [7/10], Train Loss: 0.6174, Train Acc: 80.98% | Test Loss: 0.5242, Test Acc: 83.17%\n",
      "Epoch [8/10], Train Loss: 0.6084, Train Acc: 81.00% | Test Loss: 0.5349, Test Acc: 82.00%\n",
      "Epoch [9/10], Train Loss: 0.5838, Train Acc: 81.71% | Test Loss: 0.5210, Test Acc: 83.42%\n",
      "Epoch [10/10], Train Loss: 0.5852, Train Acc: 81.58% | Test Loss: 0.5033, Test Acc: 83.75%\n"
     ]
    }
   ],
   "source": [
    "avg_training_loss, avg_testing_loss = [], []\n",
    "training_accuracies, testing_accuracies = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- TRAINING ----\n",
    "    model.train()\n",
    "    training_loss_count = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for train_feature, train_label in train_dataloader_set:\n",
    "        train_feature = train_feature.to(device)\n",
    "        train_label   = train_label.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_prediction = model(train_feature)\n",
    "        \n",
    "        # Loss\n",
    "        losses = loss_fxn(y_prediction, train_label)\n",
    "        training_loss_count += losses.item()\n",
    "        \n",
    "        # Accuracy calculation (for multiclass)\n",
    "        _, predicted = torch.max(y_prediction, 1)  #_, predicted = torch.max(output, 1) → highest probability class index nikalta hai.\n",
    "        correct_train += (predicted == train_label).sum().item()\n",
    "        total_train   += train_label.size(0)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizers.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizers.step()\n",
    "    \n",
    "    avg_train_loss = training_loss_count / len(train_dataloader_set)\n",
    "    train_accuracy = (correct_train / total_train) * 100 #accuracy training calculation\n",
    "    avg_training_loss.append(avg_train_loss)\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # ---- TESTING ----\n",
    "    model.eval()\n",
    "    testing_loss_count = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_label in test_dataloader_set:\n",
    "            test_feature = test_feature.to(device)\n",
    "            test_label   = test_label.to(device)\n",
    "            \n",
    "            prediction = model(test_feature)\n",
    "            test_loss = loss_fxn(prediction, test_label)\n",
    "            testing_loss_count += test_loss.item()\n",
    "            \n",
    "            # Accuracy calculation\n",
    "            _, predicted = torch.max(prediction, 1)\n",
    "            correct_test += (predicted == test_label).sum().item()\n",
    "            total_test   += test_label.size(0)\n",
    "    \n",
    "    avg_test_loss = testing_loss_count / len(test_dataloader_set)\n",
    "    test_accuracy = (correct_test / total_test) * 100\n",
    "    avg_testing_loss.append(avg_test_loss)\n",
    "    testing_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% \"\n",
    "          f\"| Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbcad26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b8334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982524d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f3732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183c43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
